This study is about placing the words in a read text into a hash table. Chaining method was used as the collusion resolution strategy used.
This study was written in NetBeans environment and contains 4 classes. These classes are Chain, Hash, Hash_Interface and Main, respectively.
To briefly describe these classes, the Chain class is a class that contains the necessary
variables for creating chains to be used in the chaining method. These variables are key and next, respectively. In the Hash_Interface class, there is a blueprint of the methods found in the Hash class. The Hash class, on the other hand, contains methods for extracting words from text, creating a hash function, finding a word and its frequency from the hash table, finding the total collision count, and calculating the efficiency of the table. The Main class is the main class where the methods in the Hash class are executed.
To talk about the methods used in the Hash class:
The GetHash method is a function that generates the key value necessary for adding words from the text to the hash table. To achieve this, the method iterates through the letters of the desired word to find the key value. During this process, the key value is updated for each letter. The key value obtained from the loop is taken its absolute. The reason for taking the absolute value is to eliminate the possibility of a negative result. The resulting key value is further taken modulo the table size to obtain its final form. The modulo operation by the table size is performed to prevent the key value from exceeding the table size. If it does
exceed, it cannot be placed in the table.
In the ReadFileandGenerateHash method, words are obtained from the text and placed in the table. In the first stage, from each non-empty line, words devoid of spaces and special characters are obtained and stored in an array called "words." Two additional arrays are then generated from this array: one is "wordsArray," encompassing all existing words, and the other is "uniqueWordsArray," containing only unique words. This means that it includes only one instance of each word, even if there are multiple occurrences. These arrays are created for use in other methods and calculating efficiency. In the second stage, the key value for the current word is calculated using the GetHash method. These calculated key values also represent the index values in the table. If there is no word at this index, the current word is placed at the existing index, resulting in the formation of a chain at that index. If the index is occupied, the word is added to the end of the chain. If the added word is not present in the chain, the collision count is increased. If it is present, the collision count remains constant.
The user is alerted with error messages in case the file is not found or encounters an error while reading.
There are two DisplayResult methods available. In the first one, the frequency values of the words in the text are written to a file specified by the given path. If the file at the specified path already exists, it directly writes these values to the file. If it doesn't exist, a new file with the same name is created at the specified file location, and the writing process is carried out.
If an error occurs during writing to the file, the user is informed with a message. The uniqueWordArray is used to iterate through the words in the text. This is because the array
 
contains only one instance of each word, preventing multiple outputs for the same word. By traversing this array, each word and its frequency are determined using the showFrequency method. Subsequently, the word and its frequency value are written to the file. In the second method with the same name, the only difference is that these values are written to the console instead of a file.
In the DisplayResultOrdered method, the words and their frequency values from the text are written to a file in descending order based on frequency, from the word with the highest frequency to the word with the lowest frequency. The file creation stage is the same as in the previous method. In this method, the words from the uniqueWordArray are used. A bubble sort algorithm is employed for sorting, determining the order of words to be written based on the magnitude of their frequency values. This ensures that words are written in order from the highest to the lowest frequency. If there is an error during the file writing process, the user is informed with a message.
In the showFrequency method, the word for which the frequency is to be determined is traversed throughout the hash table. This search is conducted through chains. Each word in the chains is compared with the desired word. If they are the same, the value of the variable holding the frequency is incremented. After scanning the table, the method returns this frequency value.
In the showMaxRepeatedWord method, the word with the highest frequency in the text is determined. The word at the first index of the uniqueWordArray array is considered the word with the maximum frequency. Subsequently, this word is sequentially compared with all
other words in the array. If a word with a larger frequency is encountered, that word is then considered to have the highest frequency. At the end of the loop, the word with the highest frequency is found.
In the checkWord method, which takes a word as a parameter, it determines whether the word is present in the text, how many occurrences there are, and the first position it appears in. To accurately determine the order, the wordsArray array containing all the words is used. The desired word is compared with all the words in this array. If it is found, the frequency of this word is determined using the showFrequency method. The checkValue variable is used to determine its position, and it is incremented by 1 with each comparison. When the word is found, the value in this variable indicates the first position in the text where the word occurs. If the word is not found, a message indicating that it was not found is sent.
In the TestEfficiency method, the efficiency of the hash table is calculated. Collusion rate is defined as the ratio of the total collision count to the total number of words. This ratio represents the percentage of existing words that share the same index value with another word. In other words, if there are two words at the same index, the collision value for that index is considered as one. Efficiency is found by subtracting the collusion ratio from hundred.


The NumberOfCollusion method returns the total number of collisions in the hash table.
 
In a hash table, the scarcity of collisions, the efficiency of the table, is crucial. The low occurrence of collisions depends on establishing a hash function where the key values of distinct words are also distinct, and it is influenced by the size of the table.
The table size to be used here is determined according to the load factor. An attempt has
been made to keep the load factor between 0.7 and 0.8. A load factor value within this range ensures that the table is adequately filled, while also helping to control collisions. The table size is obtained by dividing the total number of data by the load factor. In this study, a table size has been selected to achieve a load factor of 0.75, given that the total data count is 519.
Consequently, the table size is set to 692, resulting in 62 collisions. As this size value
decreases, the load factor decreases and the collision count increases. For example, when the table size is 200, the collision count becomes 155. Conversely, as the table size increases, the load factor increases and the collision rate decreases. However, in this case, the table may not be adequately filled. For instance, when the table size is 1200, the collision count is 37.
The value obtained in the loop in the hash function is multiplied by the length of the word each time, and this result is then added to the ASCII value of the next character in the word. The reason for using the length of the word in this function is to establish a more specific hash function.
To test the dynamism of the study, random texts were used. For example, in a text with 1038 words, when transferring the words to a table with a size of 1466, the collision count was 50, and when the size was increased to 2000, the collision count reduced to 38. Another example, in a text with 294 words, when transferring the words to a table with a size of 392, the collision count was 43, and when the size was increased to 600, the collision count reduced to 33. For these datasets, the initial table values tested are also set with a load
factor of 0.75.
As a result, in this study, the chaining method is used to deal with collisions that occur in the hash table. The variation in the collision count was observed based on the table size and the chosen hash function. To demonstrate the dynamism of the study, different datasets were
employed, and it was observed to be dynamic.
